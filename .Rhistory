m <<- NULL
}
get <- function() x
setinverse <- function(inverse) m <<- inverse
getinverse <- function() m
list(set = set, get = get,
setinverse = setinverse,
getinverse = getinverse)
}
cacheSolve <- function(x, ...) {
m <- x$getinverse()
if(!is.null(m)) {
message("getting cached data")
return(m)
}
data <- x$get()
m <- solve(data, ...)
x$setinverse(m)
m
}
b <- makeCacheMatrix()
str(b)
amatrix <- matrix(c(1, 2, 3, 4), nrow = 2,  ncol = 2)  # create an invertible matrix
amatrix
b$set(amatrix)
b$get()
cacheSolve(b)
cacheSolve(b)
makeCacheMatrix <- function(x = matrix()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setinverse <- function(inverse) m <<- inverse
getinverse <- function() m
list(set = set, get = get,
setinverse = setinverse,
getinverse = getinverse)
}
cacheSolve <- function(x, ...) {
m <- x$getinverse()
if(!is.null(m)) {
message("getting cached data")
return(m)
}
data <- x$get()
m <- solve(data, ...)
x$setinverse(m)
m
}
b <- makeCacheMatrix()
amatrix <- matrix(c(1, 2, 3, 4), nrow = 2,  ncol = 2)  # create an invertible matrix
amatrix
#use v's set function to create a vector
#  containing the numbers 20 through to 40
b$set(amatrix)
#use v's get function to retrieve the vector created
b$get()
#pass the list v to the cachemean() function
#   the mean of the numeric vector 20:40 should be returned
cacheSolve(b)
## Caching the Inverse of a Matrix
## This function creates a special "matrix" object that can cache its inverse
makeCacheMatrix <- function(x = matrix()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setinverse <- function(inverse) m <<- inverse
getinverse <- function() m
list(set = set, get = get,
setinverse = setinverse,
getinverse = getinverse)
}
## This function computes the inverse of the special "matrix"
##returned by makeCacheMatrix above.
##If the inverse has already been calculated (and the matrix has not changed)
##, then the cachesolve should retrieve the inverse from the cache
cacheSolve <- function(x, ...) {
m <- x$getinverse()
if(!is.null(m)) {
message("getting cached data")
return(m)
}
data <- x$get()
m <- solve(data, ...)
x$setinverse(m)
m
## Return a matrix that is the inverse of 'x'
}
b <- makeCacheMatrix()
amatrix <- matrix(c(1, 2, 3, 4), nrow = 2,  ncol = 2)  # create an invertible matrix
amatrix
#use v's set function to create a vector
#  containing the numbers 20 through to 40
b$set(amatrix)
#use v's get function to retrieve the vector created
b$get()
#pass the list v to the cachemean() function
#   the mean of the numeric vector 20:40 should be returned
cacheSolve(b)
set.seed(1)
rpois(5, 2)
set.seed(1)
rpois(5, 2)
set.seed(1)
rpois(5, 2)
?rpois
?dpois
?qpois
set.seed(10)
x <- rbinom(10, 10, 0.5)
e <- rnorm(10, 0, 20)
y <- 0.5 + 2 * x + e
y
x
e
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
dataBank<-read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat", sep=" ")
#Visualizar un resumen de los datos
summary(dataBank)
labels <- dataBank[,15]
data <- dataBank[,1:14]
#Calcular el análisis de componentes principales
pca <- princomp(data)
print(cumsum(pca$dev)/sum(pca$dev))
cumsum(pca$dev)/sum(pca$dev)
cumsum(pca$sdev)/sum(pca$sdev)
print(cumsum(pca$sdev)/sum(pca$sdev))
scores <- pca$scores
plot(scores[,1],scores[,2])
scores <- pca$scores
plot(scores[,1],scores[,2])
plot(scores[labels==1,1],scores[labels==1,2],col="red")
points(scores[labels==0,1],scores[labels==0,2],col="green")
library(rpart);
library(rpart.plot);
library(e1071);
# leer el conjunto de datos directamente de la fuente
dataRaw <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data", sep=",");
# generamos un conjunto de test tomando uno de cada tres elementos del conjunto original
N <- dim(dataRaw)[1];
all <- seq(1, N);
test <- seq(1, N, 3);
train <- setdiff(all, test);
xtrain <- dataRaw[train,];
xtest <- dataRaw[test,];
library(rpart);
library(rpart.plot);
library(e1071);
# leer el conjunto de datos directamente de la fuente
dataRaw <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data", sep=",");
# generamos un conjunto de test tomando uno de cada tres elementos del conjunto original
N <- dim(dataRaw)[1];
all <- seq(1, N);
test <- seq(1, N, 3);
train <- setdiff(all, test);
xtrain <- dataRaw[train,];
xtest <- dataRaw[test,];
table(xtrain,xtest)
View(dataRaw)
sum(xtrain[,2] == 'N') * 100 / nrow(xtrain)
sum(xtest[,2] == 'N') * 100 / nrow(xtest)
table(xtrain$V2)
table(xtest$V2)
136 /(136+243)
76/(114+76)
fvars <= paste("xtrainV2 ",paste(paste("xtrainV",3:32,sep=""),collapse="+"));
t1 <= rpart(fvars, method="class");
fvars <= paste("xtrain$V2 ",paste(paste("xtrain$V",3:32,sep=""),collapse="+"));
t1 <= rpart(fvars, method="class");
fvars <- paste("V2 ~", paste(paste("V",3:32,sep=""),collapse="+"));
t1 <- rpart(fvars, data=xtrain, method="class");
y1train <- predict(t1,xtrain, type="class")
# Generamos una matriz de confusión
tab.train <- table(xtrain$V2,y1train)
tab.train
# Calculamos el porcentaje de aciertos
(tab.train[1,1]+tab.train[2,2]) * 100 / sum(tab.train)
(239+129)/379
y1test <- predict(t1,xtest, type="class")
# Generamos una matriz de confusión
tab.test <- table(xtest$V2,y1test)
tab.test
# Calculamos el porcentaje de aciertos
(tab.test[1,1]+tab.test[2,2]) * 100 / sum(tab.test)
c <- rpart.control(minsplit=0, cp=0.0)
t2 <- rpart(fvars, data=xtrain, method="class", control=c);
prp(t2, extra=1)
prp(t2, extra=1)
y2test <- predict(t2,xtest, type="class")
tab2.test <- table(xtest$V2,y2test)
(tab2.test[1,1]+tab2.test[2,2]) * 100 / sum(tab2.test)
install.packages('e1071')
# Una vez instalado y siempre que queráis repetir la práctica, deberéis cargar primero la biblioteca e1071.
library(e1071)
install.packages("e1071")
#Base de datos de ejemplo, con 30 atributos numéricos sobre pacientes de cáncer
# Opción de recuperar datos de Internet
dataRaw <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data", sep=",");
View(dataRaw)
View(dataRaw)
summary(dataRaw)
head(dataRaw)
labels <- dataRaw[,2]
data <- dataRaw[,3:32]
NObs <-  nrow(data)
NTrain <- round(NObs*0.9)
NTest <- NObs - NTrain
train <- data[1:NTrain,]
labelsTrain <- labels[1:NTrain]
test <- data[(NTrain+1):NObs,]
labelsTest <- labels[(NTrain+1):NObs]
pca <- princomp(train)
train <- predict(pca,train)[,1:2]
test <- predict(pca, test)[,1:2]
trainFact=data.frame(train,y=as.factor(labelsTrain))
svmfit=svm(y~.,data=trainFact,kernel="linear",cross = 5)
library(e1071)
trainFact=data.frame(train,y=as.factor(labelsTrain))
svmfit=svm(y~.,data=trainFact,kernel="linear",cross = 5)
out=predict(svmfit,train)
NTrain
print(sum(out==labelsTrain))/NTrain
out=predict(svmfit,test)
NTest
print(sum(out==labelsTest))/NTest
#Función Kernel Radial
svmfit=svm(y~.,data=trainFact,kernel="radial",cross=5)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
svmfit=svm(y~.,data=trainFact,kernel="polynomial",cross=5)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
#Función Kernel Sinoidal
svmfit=svm(y~.,data=trainFact,kernel="sigmoid",cross=5)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
out=predict(svmfit,train)
NTrain
print(sum(out==labelsTrain))/NTrain
###################
# Ejercicio Práctico
###################
#Instalar la libreria en caso de que no exista con anterioridad
#install.packages("e1071")
#Cargar a librería
library(e1071)
# Lectura de los datos a utilizar
#Base de datos de ejemplo, con 32 atributos numéricos sobre pacientes de cancer
dataRaw <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data", sep=",");
#Visualizar un resumen de los datos
summary(dataRaw)
#Separamos los datos de las etiquetas
labels <- dataRaw[,2]
data <- dataRaw[,3:32]
#Ejercicio 10
#Definir el conjunto de entrenamiento:
NObs <-  nrow(data)
NTrain <- round(NObs*0.9)
NTest <- NObs - NTrain
train <- data[1:NTrain,]
labelsTrain <- labels[1:NTrain]
#Definir el conjunto de test: escogemos los siguientes pacientes
test <- data[(NTrain+1):NObs,]
labelsTest <- labels[(NTrain+1):NObs]
#Proyectamos los datos en las primeras dos componentes principales
pca <- princomp(train)
train <- predict(pca,train)[,1:2]
test <- predict(pca, test)[,1:2]
#Dibujamos los datos de entrenamiento proyectados
plot(train[labelsTrain=="B",1],train[labelsTrain=="B",2],col="red")
points(train[labelsTrain=="M",1],train[labelsTrain=="M",2],col="green")
#Dibujamos los datos de test proyectados
points(test[labelsTest=="B",1],test[labelsTest=="B",2],col="cyan")
points(test[labelsTest=="M",1],test[labelsTest=="M",2],col="black")
#Entrenamos una SVM Lineal
trainFact=data.frame(train,y=as.factor(labelsTrain))
svmfit=svm(y~.,data=trainFact,kernel="linear",cross = 5)
print(svmfit)
#Predecir el resultado del propio conjunto de entrenamiento
out=predict(svmfit,train)
print(sum(out==labelsTrain))/NTrain
plot(svmfit,trainFact)
plot(svmfit,trainFact)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
svmfit=svm(y~.,data=trainFact,kernel="radial",cross=5)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
plot(svmfit,trainFact)
svmfit=svm(y~.,data=trainFact,kernel="polynomial",cross=5)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
svmfit=svm(y~.,data=trainFact,kernel="sigmoid",cross=5)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
install.packages('ggplot')
install.packages('ggplot2')
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
install.packages('ggplot2')
library('ggplot2')
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
title: "Untitled"
setwd("~/Google Drive/Cursos/Data Science/5_Reproducible Research/Week2/PeerAssessment1/RepData_PeerAssessment1")
library(knitr)
```
---
title: "PA1_template"
author: "Diego Garzon"
date: "14 de abril de 2015"
output: html_document
---
# Reproducible Research: Peer Assessment 1
## Loading and preprocessing the data
```{r loaddata}
unzip(zipfile="activity.zip")
data <- read.csv("activity.csv")
```
## What is mean total number of steps taken per day?
```{r}
library(ggplot2)
total.steps <- tapply(data$steps, data$date, FUN=sum, na.rm=TRUE)
qplot(total.steps, binwidth=1000, xlab="total number of steps taken each day")
mean(total.steps, na.rm=TRUE)
median(total.steps, na.rm=TRUE)
```
## What is the average daily activity pattern?
```{r}
library(ggplot2)
averages <- aggregate(x=list(steps=data$steps), by=list(interval=data$interval),
FUN=mean, na.rm=TRUE)
ggplot(data=averages, aes(x=interval, y=steps)) +
geom_line() +
xlab("5-minute interval") +
ylab("average number of steps taken")
```
On average across all the days in the dataset, the 5-minute interval contains
the maximum number of steps?
```{r}
averages[which.max(averages$steps),]
```
## Imputing missing values
There are many days/intervals where there are missing values (coded as `NA`). The presence of missing days may introduce bias into some calculations or summaries of the data.
```{r how_many_missing}
missing <- is.na(data$steps)
# How many missing
table(missing)
```
All of the missing values are filled in with mean value for that 5-minute
interval.
```{r}
# Replace each missing value with the mean value of its 5-minute interval
fill.value <- function(steps, interval) {
filled <- NA
if (!is.na(steps))
filled <- c(steps)
else
filled <- (averages[averages$interval==interval, "steps"])
return(filled)
}
filled.data <- data
filled.data$steps <- mapply(fill.value, filled.data$steps, filled.data$interval)
```
Now, using the filled data set, let's make a histogram of the total number of steps taken each day and calculate the mean and median total number of steps.
```{r}
total.steps <- tapply(filled.data$steps, filled.data$date, FUN=sum)
qplot(total.steps, binwidth=1000, xlab="total number of steps taken each day")
mean(total.steps)
median(total.steps)
```
Mean and median values are higher after imputing missing data. The reason is
that in the original data, there are some days with `steps` values `NA` for
any `interval`. The total number of steps taken in such days are set to 0s by
default. However, after replacing missing `steps` values with the mean `steps`
of associated `interval` value, these 0 values are removed from the histogram
of total number of steps taken each day.
## Are there differences in activity patterns between weekdays and weekends?
First, let's find the day of the week for each measurement in the dataset. In
this part, we use the dataset with the filled-in values.
```{r}
weekday.or.weekend <- function(date) {
day <- weekdays(date)
if (day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"))
return("weekday")
else if (day %in% c("Saturday", "Sunday"))
return("weekend")
else
stop("invalid date")
}
filled.data$date <- as.Date(filled.data$date)
filled.data$day <- sapply(filled.data$date, FUN=weekday.or.weekend)
```
Now, let's make a panel plot containing plots of average number of steps taken
on weekdays and weekends.
```{r}
averages <- aggregate(steps ~ interval + day, data=filled.data, mean)
ggplot(averages, aes(interval, steps)) + geom_line() + facet_grid(day ~ .) +
xlab("5-minute interval") + ylab("Number of steps")
```
unzip(zipfile="activity.zip")
data <- read.csv("activity.csv")
unzip(zipfile="activity.zip")
data <- read.csv("activity.csv")
library(ggplot2)
total.steps <- tapply(data$steps, data$date, FUN=sum, na.rm=TRUE)
qplot(total.steps, binwidth=1000, xlab="total number of steps taken each day")
mean(total.steps, na.rm=TRUE)
median(total.steps, na.rm=TRUE)
library(ggplot2)
averages <- aggregate(x=list(steps=data$steps), by=list(interval=data$interval),
FUN=mean, na.rm=TRUE)
ggplot(data=averages, aes(x=interval, y=steps)) +
geom_line() +
xlab("5-minute interval") +
ylab("average number of steps taken")
averages[which.max(averages$steps),]
missing <- is.na(data$steps)
# How many missing
table(missing)
fill.value <- function(steps, interval) {
filled <- NA
if (!is.na(steps))
filled <- c(steps)
else
filled <- (averages[averages$interval==interval, "steps"])
return(filled)
}
filled.data <- data
filled.data$steps <- mapply(fill.value, filled.data$steps, filled.data$interval)
total.steps <- tapply(filled.data$steps, filled.data$date, FUN=sum)
qplot(total.steps, binwidth=1000, xlab="total number of steps taken each day")
mean(total.steps)
median(total.steps)
weekday.or.weekend <- function(date) {
day <- weekdays(date)
if (day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"))
return("weekday")
else if (day %in% c("Saturday", "Sunday"))
return("weekend")
else
stop("invalid date")
}
filled.data$date <- as.Date(filled.data$date)
filled.data$day <- sapply(filled.data$date, FUN=weekday.or.weekend)
averages <- aggregate(steps ~ interval + day, data=filled.data, mean)
ggplot(averages, aes(interval, steps)) + geom_line() + facet_grid(day ~ .) +
xlab("5-minute interval") + ylab("Number of steps")
weekday.or.weekend <- function(date) {
day <- weekdays(date)
if (day %in% c("Lunes", "Martes", "Miercoles", "Jueves", "Viernes"))
return("weekday")
else if (day %in% c("Sabado", "Domingo"))
return("weekend")
else
stop("invalid date")
}
filled.data$date <- as.Date(filled.data$date)
filled.data$day <- sapply(filled.data$date, FUN=weekday.or.weekend)
weekday.or.weekend <- function(date) {
day <- weekdays(date)
if (day %in% c("Lunes", "Martes", "Miércoles", "Jueves", "Viernes"))
return("weekday")
else if (day %in% c("Sabado", "Domingo"))
return("weekend")
else
stop("invalid date")
}
filled.data$date <- as.Date(filled.data$date)
filled.data$day <- sapply(filled.data$date, FUN=weekday.or.weekend)
weekday.or.weekend <- function(date) {
day <- weekdays(date)
if (day %in% c("Lunes", "Martes", "Miércoles", "Jueves", "Viernes"))
return("weekday")
else if (day %in% c("Sábado", "Domingo"))
return("weekend")
else
stop("invalid date")
}
filled.data$date <- as.Date(filled.data$date)
filled.data$day <- sapply(filled.data$date, FUN=weekday.or.weekend)
